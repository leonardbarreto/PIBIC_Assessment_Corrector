{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar conjunto de documentos do diretório\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "path=\"/home/leonard/Faculdades/ESTACIO/Projetos/IC/2017-2018/Luiz/Dados/SistMicroprocessadores\"\n",
    "def encontraArquivosEmPastaRecursivamente(pasta, extensao):\n",
    "    arquivosTxt = []\n",
    "    caminhoAbsoluto = os.path.abspath(pasta)\n",
    "    for pastaAtual, subPastas, arquivos  in os.walk(caminhoAbsoluto):\n",
    "        arquivosTxt.extend([os.path.join(pastaAtual,arquivo) for arquivo in arquivos if arquivo.endswith('.txt')])\n",
    "    return arquivosTxt\n",
    "\n",
    "\n",
    "arquivos = encontraArquivosEmPastaRecursivamente(path, '.txt')\n",
    "\n",
    "'''\n",
    "Padrão do arquivo\n",
    "1a linha: <nota atribuída> de <valor da questão>\n",
    "2a linha: em branco\n",
    "3a linha em diante: resposta\n",
    "'''\n",
    "corpus_respostas=[]\n",
    "for a in arquivos:\n",
    "    arq = open(a,'r')\n",
    "    with open(a) as lines:\n",
    "        #lines = arq.readlines()\n",
    "        lines = [lines.rstrip(\"\\n\") for lines in arq] #retirar o caracter de fim de linha\n",
    "        # then skip the next 2\n",
    "        for line in islice(lines, 3):\n",
    "            pass\n",
    "        corpus_respostas.append(line)\n",
    "    arq.close()\n",
    "\n",
    "print(len(corpus_respostas))\n",
    "#print(corpus_respostas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O desvio no programa ocorre sempre que o programa sai da sequência normal de execução. Existem basicamente dois tipos de desvio: desvio incondicional e desvio condicional. O desvio incondicional ocorre quando o programa sai da sequência normal de execução e é desviado para outro ponto da memória de programa independente de qualquer condição; o desvio ocorre sem depender de nada. Um exemplo de um desvio incondicional é aquele provocado pela instrução GOTO. No desvio condicional, uma condição deve ser satisfeita para que o desvio ocorra. Normalmente uma determinada condição é testada e, se o teste for validado, ocorre o desvio.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#O gabarito está na posição 0\n",
    "gabarito = \"O desvio no programa ocorre sempre que o programa sai da sequência normal de execução. Existem basicamente dois tipos de desvio: desvio incondicional e desvio condicional. O desvio incondicional ocorre quando o programa sai da sequência normal de execução e é desviado para outro ponto da memória de programa independente de qualquer condição; o desvio ocorre sem depender de nada. Um exemplo de um desvio incondicional é aquele provocado pela instrução GOTO. No desvio condicional, uma condição deve ser satisfeita para que o desvio ocorra. Normalmente uma determinada condição é testada e, se o teste for validado, ocorre o desvio.\"\n",
    "respostas = corpus_respostas\n",
    "respostas.insert(0,gabarito)\n",
    "respostas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-processamento\n",
    "=====\n",
    "1. remoção de stopwords\n",
    "2. remoção de pontuações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retirar pontuação\n",
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "#print(strip_punctuation(respostas[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retirar stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w.lower() for w in text.split() if w not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retirar caracteres especiais\n",
    "import unicodedata\n",
    "import re\n",
    "def remove_special_chars(text):\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    palavraSemAcento = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    return re.sub(u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ: ]', '', palavraSemAcento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef merge_two_dicts(x, y):\\n    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\\n    z = x.copy()\\n    z.update(y)\\n    return z\\n\\ndef ngrams_split(lst, n):\\n    counts = dict()\\n    grams = [\\' \\'.join(lst[i:i+n]) for i in range(len(lst)-n)]\\n    for gram in grams:\\n        if gram not in counts:\\n            counts[gram] = 1\\n        else:\\n            counts[gram] += 1\\n    return counts\\n\\ngabarito_2grams = ngrams_split(gabarito.split(),2)\\ngabarito_3grams = ngrams_split(gabarito.split(),3)\\nngram=merge_two_dicts(gabarito_3grams,gabarito_2grams)\\nngram=merge_two_dicts(ngram,ngrams_split(gabarito.split(),1))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk import ngrams \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#gabarito = 'O desvio no programa ocorre sempre que o programa sai da sequência normal de execução. Existem basicamente dois tipos de desvio: desvio incondicional e desvio condicional. O desvio incondicional ocorre quando o programa sai da sequência normal de execução e é desviado para outro ponto da memória de programa independente de qualquer condição; o desvio ocorre sem depender de nada. Um exemplo de um desvio incondicional é aquele provocado pela instrução GOTO. No desvio condicional, uma condição deve ser satisfeita para que o desvio ocorra. Normalmente uma determinada condição é testada e, se o teste for validado, ocorre o desvio.'\n",
    "\n",
    "def word_grams(words, min=1, max=3):\n",
    "    s = []\n",
    "    if isinstance(words,str): \n",
    "        words=words.split()\n",
    "    \n",
    "    for n in range(min, max+1):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "'''\n",
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def ngrams_split(lst, n):\n",
    "    counts = dict()\n",
    "    grams = [' '.join(lst[i:i+n]) for i in range(len(lst)-n)]\n",
    "    for gram in grams:\n",
    "        if gram not in counts:\n",
    "            counts[gram] = 1\n",
    "        else:\n",
    "            counts[gram] += 1\n",
    "    return counts\n",
    "\n",
    "gabarito_2grams = ngrams_split(gabarito.split(),2)\n",
    "gabarito_3grams = ngrams_split(gabarito.split(),3)\n",
    "ngram=merge_two_dicts(gabarito_3grams,gabarito_2grams)\n",
    "ngram=merge_two_dicts(ngram,ngrams_split(gabarito.split(),1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tabulate import tabulate\\nimport pandas as pd\\ndef printf(lst):\\n    df = pd.DataFrame(list(lst))\\n    print(tabulate(df,headers='keys', tablefmt='psql'))\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "def printf(lst):\n",
    "    df = pd.DataFrame(list(lst))\n",
    "    print(tabulate(df,headers='keys', tablefmt='psql'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_processamento(lst):\n",
    "    item=[]\n",
    "    for idx,val in enumerate(lst):\n",
    "        val=val.lower()\n",
    "        s=strip_punctuation(val)\n",
    "        s=remove_stopwords(val)\n",
    "        s=remove_special_chars(val)\n",
    "\n",
    "        item.append(s)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words pacote sklearn \n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# PRÉ-PROCESSAMENTO\n",
    "respostas = pre_processamento(respostas)\n",
    "#print(respostas)\n",
    "\n",
    "######################\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = word_grams, preprocessor = None, stop_words = stopwords, max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(respostas)\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "#print(len(vocab))\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print(count, tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar a similaridade do 1o registro (resposta) com os demais\n",
    "=====\n",
    "### Método Cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.07455478, 0.25147277, 0.18194353,\n",
       "        0.39304295, 0.5342046 , 0.79191791, 0.0952597 , 0.73975897,\n",
       "        1.        , 0.72415576, 0.67752697, 0.0906888 , 0.03390318,\n",
       "        0.65329111, 0.23529553]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Similaridade entre o 1 documento (que é o gabarito) e os demais\n",
    "cosine_similarity(train_data_features[0:1],train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Distância Euclideana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 21.11871208, 23.08679276, 20.90454496, 21.28379665,\n",
       "        19.28730152, 17.63519209, 12.76714533, 21.49418526, 14.14213562,\n",
       "         0.        , 14.38749457, 15.3622915 , 23.40939982, 21.42428529,\n",
       "        16.55294536, 20.37154879]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "euclidean_distances(train_data_features[0:1],train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Linear Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[435.,   0.,  18.,  56.,  36.,  84., 120., 258.,  15., 265., 435.,\n",
       "        223., 188.,  24.,   4., 118.,  34.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "linear_kernel(train_data_features[0:1],train_data_features)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
